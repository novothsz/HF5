{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyperas_main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEJZ3BJzv9US",
        "outputId": "3c16e709-d445-451f-ab0d-e6c5539d1d08"
      },
      "source": [
        "# Baseline model with increasing dropout on the cifar dataset\n",
        "\n",
        "# source: https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\n",
        "\n",
        "\n",
        "# érdemes előre telepíteni, és utána újraindítani\n",
        "!pip3 install hyperas\n",
        "!pip3 install hyperopt\n",
        "\n",
        "\n",
        "import sys\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hyperas in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hyperas) (1.0.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.4.3)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.1.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.6.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.0.8)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.3.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.0.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (7.5.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.10.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.4.1)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (3.11.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (2.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (4.41.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.3.3)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.11.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.7.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.4.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (3.2.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.6.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.6.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (5.3.5)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (1.0.18)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (5.1.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.9.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (1.5.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->hyperas) (20.0.0)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->hyperas) (1.9.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (3.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt->hyperas) (4.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->hyperas) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (20.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->jupyter-console->jupyter->hyperas) (2.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (50.3.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->hyperas) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->hyperas) (0.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->bleach->nbconvert->hyperas) (2.4.7)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.5)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.11.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf309WPVBDgV"
      },
      "source": [
        "# Load train and test dataset\n",
        "def load_dataset():\n",
        "  # Load dataset\n",
        "  (trainX, trainY),(testX, testY) = cifar10.load_data()\n",
        "  # One hot encode target values\n",
        "  trainY = to_categorical(trainY)\n",
        "  testY = to_categorical(testY)\n",
        "\n",
        "  # Convert from integers to floats\n",
        "  trainX = trainX.astype('float32')\n",
        "  testX = testX.astype('float32')\n",
        "  # normalize to range 0-1\n",
        "  trainX = trainX / 255.0\n",
        "  testX = testX / 255.0\n",
        "\n",
        "  train_x = trainX\n",
        "  train_y = trainY\n",
        "  test_x = testX\n",
        "  test_y = testY\n",
        "\n",
        "  # return trainX, trainY, testX, testY\n",
        "  return train_x, train_y, test_x, test_y\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c7t7UKewnWC"
      },
      "source": [
        "def create_model(train_x, train_y, test_x, test_y):\n",
        "  from keras.layers import Layer\n",
        "  from keras import backend as K\n",
        "  import numpy as np\n",
        "\n",
        "  n_filters1 = {{choice([16, 32, 64, 128, 256])}}\n",
        "  n_filters2 = {{choice([16, 32, 64, 128, 256])}}\n",
        "  # n_filters1 = {{choice([32])}}\n",
        "  # n_filters2 = {{choice([32])}}\n",
        "  n_kernel1 = {{choice([3, 5, 7])}}\n",
        "  n_kernel2 = {{choice([3, 5, 7])}}\n",
        "  dropout_1 = {{uniform(0, 0.5)}}\n",
        "  dropout_2 = {{uniform(0, 0.5)}}\n",
        "  n_batch = {{choice([64, 128, 256])}}\n",
        "  optim = {{choice(['rmsprop', 'adam', 'sgd'])}}\n",
        "  \n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(\n",
        "      filters = n_filters1, \n",
        "      kernel_size = n_kernel1, \n",
        "      activation = 'relu', \n",
        "      padding='same', \n",
        "      input_shape = (32, 32, 3)))\n",
        "  model.add(Dropout(rate = dropout_1))\n",
        "  model.add(Conv2D(filters = n_filters2, \n",
        "                   kernel_size = n_kernel2,\n",
        "                   activation = 'relu', \n",
        "                   padding='same'))\n",
        "  model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu', padding='same'))\n",
        "  model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "  model.add(Dropout(rate = dropout_2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units = 128, activation = 'relu'))\n",
        "  model.add(Dropout(rate = 0.5))\n",
        "  model.add(Dense(units = 10, activation = 'softmax'))\n",
        "  # compile model\n",
        "  model.compile(optimizer = optim, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "  # Done :) Happy :)\n",
        "\n",
        "  batch_size = 32\n",
        "  num_classes = 10\n",
        "  epochs = 10\n",
        "  # while True:pass\n",
        "  result = model.fit(train_x, train_y,\n",
        "                     batch_size = n_batch,\n",
        "                     epochs = epochs,\n",
        "                     verbose = 2,\n",
        "                     validation_data = (test_x, test_y),\n",
        "                     shuffle = True)\n",
        "  \n",
        "  # Az epoch_ok közül a legnagyob val_acc elmentése\n",
        "  best_val_acc = np.amax(result.history['val_accuracy'])\n",
        "  print('A legjobb val_acc:', best_val_acc)\n",
        "\n",
        "  # Log kiírása: háló struktúra, és az eredmény\n",
        "  with open('hyperas-fashionmnist-log.csv', 'a') as csv_file:\n",
        "      csv_file.write(str(n_filters1) + ';')\n",
        "      csv_file.write(str(n_filters2) + ';')\n",
        "      csv_file.write(str(n_kernel1) + ';')\n",
        "      csv_file.write(str(n_kernel2) + ';')\n",
        "      csv_file.write(str(dropout_1) + ';')\n",
        "      csv_file.write(str(dropout_2) + ';')\n",
        "      csv_file.write(str(n_batch) + ';')\n",
        "      csv_file.write(str(optim) + ';')\n",
        "      csv_file.write(str(best_val_acc) + '\\n')\n",
        "\n",
        "  # Negatív val_acc, mert a hyperopt csomag mindig minimalizál\n",
        "  return {'loss': -best_val_acc, 'status': STATUS_OK, 'model': model}\n",
        "\n",
        "  "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aFhk_qrf6-eR",
        "outputId": "2f7617b9-3915-488c-eec9-c8081db55116"
      },
      "source": [
        "# Log fájl inicializálás / fejléc\n",
        "with open('hyperas-fashionmnist-log.csv', 'w') as csv_file:\n",
        "  csv_file.write('n_filters1' + ';')\n",
        "  csv_file.write('n_filters2' + ';')\n",
        "  csv_file.write('n_kernel1' + ';')\n",
        "  csv_file.write('n_kernel2' + ';')\n",
        "  csv_file.write('dropout_1' + ';')\n",
        "  csv_file.write('dropout_2' + ';')\n",
        "  csv_file.write('n_batch' + ';')\n",
        "  csv_file.write('optim' + ';')\n",
        "  csv_file.write('best_val_acc' + '\\n')\n",
        "\n",
        "# Ezután colab-ról le kell tölteni a notebook-ot és a .ipynb fájlt utána \n",
        "# visszatölteni az aktuális könyvtárba\n",
        "\n",
        "# A hyperas-hoz kapcsolódó importok\n",
        "import hyperas\n",
        "from hyperopt import  Trials, STATUS_OK, tpe\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform\n",
        "\n",
        "# A teljes hiperparaméter optimalizálás indítása\n",
        "\n",
        "best_run, best_model = optim.minimize(model = create_model,\n",
        "                                      data = load_dataset,\n",
        "                                      algo = tpe.suggest,\n",
        "                                      max_evals = 100,\n",
        "                                      notebook_name = 'Hyperas_main-22',\n",
        "                                      trials = Trials())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> Imports:\n",
            "#coding=utf-8\n",
            "\n",
            "try:\n",
            "    import sys\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from matplotlib import pyplot\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.datasets import cifar10\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.utils import to_categorical\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.optimizers import SGD\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Layer\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras import backend as K\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import hyperas\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperopt import Trials, STATUS_OK, tpe\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pandas\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import matplotlib.pyplot as plt\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from mpl_toolkits.mplot3d import Axes3D\n",
            "except:\n",
            "    pass\n",
            "\n",
            ">>> Hyperas search space:\n",
            "\n",
            "def get_space():\n",
            "    return {\n",
            "        'n_filters1': hp.choice('n_filters1', [16, 32, 64, 128, 256]),\n",
            "        'n_filters1_1': hp.choice('n_filters1_1', [16, 32, 64, 128, 256]),\n",
            "        'n_filters1_2': hp.choice('n_filters1_2', [32]),\n",
            "        'n_filters1_3': hp.choice('n_filters1_3', [32]),\n",
            "        'n_kernel1': hp.choice('n_kernel1', [3, 5, 7]),\n",
            "        'n_kernel1_1': hp.choice('n_kernel1_1', [3, 5, 7]),\n",
            "        'dropout_1': hp.uniform('dropout_1', 0, 0.5),\n",
            "        'dropout_1_1': hp.uniform('dropout_1_1', 0, 0.5),\n",
            "        'n_batch': hp.choice('n_batch', [64, 128, 256]),\n",
            "        'optim': hp.choice('optim', ['rmsprop', 'adam', 'sgd']),\n",
            "    }\n",
            "\n",
            ">>> Data\n",
            "  1: \n",
            "  2: # Load dataset\n",
            "  3: (trainX, trainY),(testX, testY) = cifar10.load_data()\n",
            "  4: # One hot encode target values\n",
            "  5: trainY = to_categorical(trainY)\n",
            "  6: testY = to_categorical(testY)\n",
            "  7: \n",
            "  8: # Convert from integers to floats\n",
            "  9: trainX = trainX.astype('float32')\n",
            " 10: testX = testX.astype('float32')\n",
            " 11: # normalize to range 0-1\n",
            " 12: trainX = trainX / 255.0\n",
            " 13: testX = testX / 255.0\n",
            " 14: \n",
            " 15: train_x = trainX\n",
            " 16: train_y = trainY\n",
            " 17: test_x = testX\n",
            " 18: test_y = testY\n",
            " 19: \n",
            " 20: # return trainX, trainY, testX, testY\n",
            " 21: \n",
            " 22: \n",
            " 23: \n",
            ">>> Resulting replaced keras model:\n",
            "\n",
            "   1: def keras_fmin_fnct(space):\n",
            "   2: \n",
            "   3: \n",
            "   4:   n_filters1 = space['n_filters1']\n",
            "   5:   n_filters2 = space['n_filters1_1']\n",
            "   6:   # n_filters1 = space['n_filters1_2']\n",
            "   7:   # n_filters2 = space['n_filters1_3']\n",
            "   8:   n_kernel1 = space['n_kernel1']\n",
            "   9:   n_kernel2 = space['n_kernel1_1']\n",
            "  10:   dropout_1 = space['dropout_1']\n",
            "  11:   dropout_2 = space['dropout_1_1']\n",
            "  12:   n_batch = space['n_batch']\n",
            "  13:   optim = space['optim']\n",
            "  14:   \n",
            "  15: \n",
            "  16:   model = Sequential()\n",
            "  17:   model.add(Conv2D(\n",
            "  18:       filters = n_filters1, \n",
            "  19:       kernel_size = n_kernel1, \n",
            "  20:       activation = 'relu', \n",
            "  21:       padding='same', \n",
            "  22:       input_shape = (32, 32, 3)))\n",
            "  23:   model.add(Dropout(rate = dropout_1))\n",
            "  24:   model.add(Conv2D(filters = n_filters2, \n",
            "  25:                    kernel_size = n_kernel2,\n",
            "  26:                    activation = 'relu', \n",
            "  27:                    padding='same'))\n",
            "  28:   model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu', padding='same'))\n",
            "  29:   model.add(MaxPooling2D(pool_size = (2, 2)))\n",
            "  30:   model.add(Dropout(rate = dropout_2))\n",
            "  31:   model.add(Flatten())\n",
            "  32:   model.add(Dense(units = 128, activation = 'relu'))\n",
            "  33:   model.add(Dropout(rate = 0.5))\n",
            "  34:   model.add(Dense(units = 10, activation = 'softmax'))\n",
            "  35:   # compile model\n",
            "  36:   model.compile(optimizer = optim, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
            "  37: \n",
            "  38:   # Done :) Happy :)\n",
            "  39: \n",
            "  40:   batch_size = 32\n",
            "  41:   num_classes = 10\n",
            "  42:   epochs = 200\n",
            "  43:   # while True:pass\n",
            "  44:   result = model.fit(train_x, train_y,\n",
            "  45:                      batch_size = n_batch,\n",
            "  46:                      epochs = epochs,\n",
            "  47:                      verbose = 2,\n",
            "  48:                      validation_data = (test_x, test_y),\n",
            "  49:                      shuffle = True)\n",
            "  50:   \n",
            "  51:   # Az epoch_ok közül a legnagyob val_acc elmentése\n",
            "  52:   best_val_acc = np.amax(result.history['val_accuracy'])\n",
            "  53:   print('A legjobb val_acc:', best_val_acc)\n",
            "  54: \n",
            "  55:   # Log kiírása: háló struktúra, és az eredmény\n",
            "  56:   with open('hyperas-fashionmnist-log.csv', 'a') as csv_file:\n",
            "  57:       csv_file.write(str(n_filters1) + ';')\n",
            "  58:       csv_file.write(str(n_filters2) + ';')\n",
            "  59:       csv_file.write(str(n_kernel1) + ';')\n",
            "  60:       csv_file.write(str(n_kernel2) + ';')\n",
            "  61:       csv_file.write(str(dropout_1) + ';')\n",
            "  62:       csv_file.write(str(dropout_2) + ';')\n",
            "  63:       csv_file.write(str(n_batch) + ';')\n",
            "  64:       csv_file.write(str(optim) + ';')\n",
            "  65:       csv_file.write(str(best_val_acc) + '\\n')\n",
            "  66: \n",
            "  67:   # Negatív val_acc, mert a hyperopt csomag mindig minimalizál\n",
            "  68:   return {'loss': -best_val_acc, 'status': STATUS_OK, 'model': model}\n",
            "  69: \n",
            "Epoch 1/200\n",
            "  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0137s vs `on_train_batch_end` time: 0.0235s). Check your callbacks.\n",
            "391/391 - 16s - loss: 2.1267 - accuracy: 0.2137 - val_loss: 1.9634 - val_accuracy: 0.3008\n",
            "\n",
            "Epoch 2/200\n",
            "391/391 - 16s - loss: 1.9058 - accuracy: 0.3174 - val_loss: 1.7835 - val_accuracy: 0.3695\n",
            "\n",
            "Epoch 3/200\n",
            "391/391 - 16s - loss: 1.7885 - accuracy: 0.3645 - val_loss: 1.6710 - val_accuracy: 0.4021\n",
            "\n",
            "Epoch 4/200\n",
            "391/391 - 16s - loss: 1.7101 - accuracy: 0.3952 - val_loss: 1.5636 - val_accuracy: 0.4531\n",
            "\n",
            "Epoch 5/200\n",
            "391/391 - 16s - loss: 1.6332 - accuracy: 0.4217 - val_loss: 1.5467 - val_accuracy: 0.4492\n",
            "\n",
            "Epoch 6/200\n",
            "391/391 - 16s - loss: 1.5744 - accuracy: 0.4385 - val_loss: 1.5008 - val_accuracy: 0.4649\n",
            "\n",
            "Epoch 7/200\n",
            "391/391 - 16s - loss: 1.5297 - accuracy: 0.4541 - val_loss: 1.4422 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 8/200\n",
            "391/391 - 16s - loss: 1.4849 - accuracy: 0.4700 - val_loss: 1.3817 - val_accuracy: 0.5089\n",
            "\n",
            "Epoch 9/200\n",
            "391/391 - 16s - loss: 1.4478 - accuracy: 0.4820 - val_loss: 1.3362 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 10/200\n",
            "391/391 - 16s - loss: 1.4138 - accuracy: 0.4941 - val_loss: 1.2974 - val_accuracy: 0.5359\n",
            "\n",
            "Epoch 11/200\n",
            "391/391 - 16s - loss: 1.3793 - accuracy: 0.5071 - val_loss: 1.2723 - val_accuracy: 0.5441\n",
            "\n",
            "Epoch 12/200\n",
            "391/391 - 16s - loss: 1.3496 - accuracy: 0.5190 - val_loss: 1.2776 - val_accuracy: 0.5459\n",
            "\n",
            "Epoch 13/200\n",
            "391/391 - 16s - loss: 1.3223 - accuracy: 0.5296 - val_loss: 1.2291 - val_accuracy: 0.5553\n",
            "\n",
            "Epoch 14/200\n",
            "391/391 - 16s - loss: 1.2969 - accuracy: 0.5341 - val_loss: 1.2119 - val_accuracy: 0.5654\n",
            "\n",
            "Epoch 15/200\n",
            "391/391 - 16s - loss: 1.2750 - accuracy: 0.5447 - val_loss: 1.1861 - val_accuracy: 0.5747\n",
            "\n",
            "Epoch 16/200\n",
            "391/391 - 16s - loss: 1.2555 - accuracy: 0.5528 - val_loss: 1.2118 - val_accuracy: 0.5727\n",
            "\n",
            "Epoch 17/200\n",
            "391/391 - 16s - loss: 1.2283 - accuracy: 0.5608 - val_loss: 1.1568 - val_accuracy: 0.5937\n",
            "\n",
            "Epoch 18/200\n",
            "391/391 - 16s - loss: 1.2094 - accuracy: 0.5702 - val_loss: 1.1530 - val_accuracy: 0.5855\n",
            "\n",
            "Epoch 19/200\n",
            "391/391 - 16s - loss: 1.1850 - accuracy: 0.5793 - val_loss: 1.1515 - val_accuracy: 0.5877\n",
            "\n",
            "Epoch 20/200\n",
            "391/391 - 16s - loss: 1.1680 - accuracy: 0.5853 - val_loss: 1.1190 - val_accuracy: 0.5987\n",
            "\n",
            "Epoch 21/200\n",
            "391/391 - 16s - loss: 1.1521 - accuracy: 0.5933 - val_loss: 1.1052 - val_accuracy: 0.6034\n",
            "\n",
            "Epoch 22/200\n",
            "391/391 - 16s - loss: 1.1262 - accuracy: 0.6007 - val_loss: 1.0865 - val_accuracy: 0.6161\n",
            "\n",
            "Epoch 23/200\n",
            "391/391 - 16s - loss: 1.1101 - accuracy: 0.6058 - val_loss: 1.0795 - val_accuracy: 0.6183\n",
            "\n",
            "Epoch 24/200\n",
            "391/391 - 16s - loss: 1.0869 - accuracy: 0.6148 - val_loss: 1.0555 - val_accuracy: 0.6250\n",
            "\n",
            "Epoch 25/200\n",
            "391/391 - 16s - loss: 1.0713 - accuracy: 0.6221 - val_loss: 1.0471 - val_accuracy: 0.6286\n",
            "\n",
            "Epoch 26/200\n",
            "391/391 - 16s - loss: 1.0522 - accuracy: 0.6288 - val_loss: 1.0602 - val_accuracy: 0.6289\n",
            "\n",
            "Epoch 27/200\n",
            "391/391 - 16s - loss: 1.0335 - accuracy: 0.6319 - val_loss: 1.0077 - val_accuracy: 0.6461\n",
            "\n",
            "Epoch 28/200\n",
            "391/391 - 16s - loss: 1.0129 - accuracy: 0.6420 - val_loss: 1.0167 - val_accuracy: 0.6434\n",
            "\n",
            "Epoch 29/200\n",
            "391/391 - 16s - loss: 0.9991 - accuracy: 0.6474 - val_loss: 0.9975 - val_accuracy: 0.6460\n",
            "\n",
            "Epoch 30/200\n",
            "391/391 - 16s - loss: 0.9744 - accuracy: 0.6548 - val_loss: 0.9832 - val_accuracy: 0.6559\n",
            "\n",
            "Epoch 31/200\n",
            "391/391 - 16s - loss: 0.9637 - accuracy: 0.6577 - val_loss: 1.0505 - val_accuracy: 0.6315\n",
            "\n",
            "Epoch 32/200\n",
            "391/391 - 16s - loss: 0.9443 - accuracy: 0.6640 - val_loss: 0.9661 - val_accuracy: 0.6645\n",
            "\n",
            "Epoch 33/200\n",
            "391/391 - 16s - loss: 0.9276 - accuracy: 0.6696 - val_loss: 0.9710 - val_accuracy: 0.6616\n",
            "\n",
            "Epoch 34/200\n",
            "391/391 - 16s - loss: 0.9110 - accuracy: 0.6782 - val_loss: 0.9472 - val_accuracy: 0.6711\n",
            "\n",
            "Epoch 35/200\n",
            "391/391 - 16s - loss: 0.8976 - accuracy: 0.6835 - val_loss: 0.9577 - val_accuracy: 0.6685\n",
            "\n",
            "Epoch 36/200\n",
            "391/391 - 16s - loss: 0.8813 - accuracy: 0.6852 - val_loss: 0.9243 - val_accuracy: 0.6793\n",
            "\n",
            "Epoch 37/200\n",
            "391/391 - 16s - loss: 0.8634 - accuracy: 0.6916 - val_loss: 0.9727 - val_accuracy: 0.6662\n",
            "\n",
            "Epoch 38/200\n",
            "391/391 - 16s - loss: 0.8516 - accuracy: 0.6970 - val_loss: 0.9379 - val_accuracy: 0.6719\n",
            "\n",
            "Epoch 39/200\n",
            "391/391 - 16s - loss: 0.8329 - accuracy: 0.7025 - val_loss: 0.9170 - val_accuracy: 0.6819\n",
            "\n",
            "Epoch 40/200\n",
            "391/391 - 16s - loss: 0.8198 - accuracy: 0.7056 - val_loss: 0.9090 - val_accuracy: 0.6864\n",
            "\n",
            "Epoch 41/200\n",
            "391/391 - 16s - loss: 0.8025 - accuracy: 0.7136 - val_loss: 0.9025 - val_accuracy: 0.6871\n",
            "\n",
            "Epoch 42/200\n",
            "391/391 - 16s - loss: 0.7923 - accuracy: 0.7163 - val_loss: 0.9158 - val_accuracy: 0.6827\n",
            "\n",
            "Epoch 43/200\n",
            "391/391 - 16s - loss: 0.7788 - accuracy: 0.7231 - val_loss: 0.8960 - val_accuracy: 0.6913\n",
            "\n",
            "Epoch 44/200\n",
            "391/391 - 16s - loss: 0.7601 - accuracy: 0.7288 - val_loss: 0.9206 - val_accuracy: 0.6830\n",
            "\n",
            "Epoch 45/200\n",
            "391/391 - 16s - loss: 0.7520 - accuracy: 0.7302 - val_loss: 0.8908 - val_accuracy: 0.6922\n",
            "\n",
            "Epoch 46/200\n",
            "391/391 - 16s - loss: 0.7396 - accuracy: 0.7354 - val_loss: 0.8820 - val_accuracy: 0.6987\n",
            "\n",
            "Epoch 47/200\n",
            "391/391 - 16s - loss: 0.7261 - accuracy: 0.7408 - val_loss: 0.8951 - val_accuracy: 0.6955\n",
            "\n",
            "Epoch 48/200\n",
            "391/391 - 16s - loss: 0.7112 - accuracy: 0.7432 - val_loss: 0.9000 - val_accuracy: 0.6918\n",
            "\n",
            "Epoch 49/200\n",
            "391/391 - 16s - loss: 0.6986 - accuracy: 0.7500 - val_loss: 0.8933 - val_accuracy: 0.6958\n",
            "\n",
            "Epoch 50/200\n",
            "391/391 - 16s - loss: 0.6871 - accuracy: 0.7553 - val_loss: 0.9046 - val_accuracy: 0.6990\n",
            "\n",
            "Epoch 51/200\n",
            "391/391 - 16s - loss: 0.6765 - accuracy: 0.7575 - val_loss: 0.8757 - val_accuracy: 0.6988\n",
            "\n",
            "Epoch 52/200\n",
            "391/391 - 16s - loss: 0.6606 - accuracy: 0.7613 - val_loss: 0.8951 - val_accuracy: 0.6975\n",
            "\n",
            "Epoch 53/200\n",
            "391/391 - 16s - loss: 0.6500 - accuracy: 0.7654 - val_loss: 0.8978 - val_accuracy: 0.7008\n",
            "\n",
            "Epoch 54/200\n",
            "391/391 - 16s - loss: 0.6351 - accuracy: 0.7724 - val_loss: 0.8986 - val_accuracy: 0.6963\n",
            "\n",
            "Epoch 55/200\n",
            "391/391 - 16s - loss: 0.6244 - accuracy: 0.7751 - val_loss: 0.9064 - val_accuracy: 0.6942\n",
            "\n",
            "Epoch 56/200\n",
            "391/391 - 16s - loss: 0.6121 - accuracy: 0.7789 - val_loss: 0.8968 - val_accuracy: 0.6987\n",
            "\n",
            "Epoch 57/200\n",
            "391/391 - 16s - loss: 0.6062 - accuracy: 0.7806 - val_loss: 0.8900 - val_accuracy: 0.7017\n",
            "\n",
            "Epoch 58/200\n",
            "391/391 - 16s - loss: 0.5945 - accuracy: 0.7843 - val_loss: 0.8940 - val_accuracy: 0.6994\n",
            "\n",
            "Epoch 59/200\n",
            "391/391 - 16s - loss: 0.5766 - accuracy: 0.7908 - val_loss: 0.8899 - val_accuracy: 0.7051\n",
            "\n",
            "Epoch 60/200\n",
            "391/391 - 16s - loss: 0.5765 - accuracy: 0.7883 - val_loss: 0.8843 - val_accuracy: 0.7061\n",
            "\n",
            "Epoch 61/200\n",
            "391/391 - 16s - loss: 0.5562 - accuracy: 0.7959 - val_loss: 0.8990 - val_accuracy: 0.7051\n",
            "\n",
            "Epoch 62/200\n",
            "391/391 - 16s - loss: 0.5477 - accuracy: 0.7986 - val_loss: 0.8958 - val_accuracy: 0.7064\n",
            "\n",
            "Epoch 63/200\n",
            "391/391 - 16s - loss: 0.5388 - accuracy: 0.8040 - val_loss: 0.9012 - val_accuracy: 0.7063\n",
            "\n",
            "Epoch 64/200\n",
            "391/391 - 16s - loss: 0.5313 - accuracy: 0.8063 - val_loss: 0.8998 - val_accuracy: 0.7024\n",
            "\n",
            "Epoch 65/200\n",
            "391/391 - 16s - loss: 0.5184 - accuracy: 0.8122 - val_loss: 0.9112 - val_accuracy: 0.7108\n",
            "\n",
            "Epoch 66/200\n",
            "391/391 - 16s - loss: 0.5113 - accuracy: 0.8134 - val_loss: 0.9127 - val_accuracy: 0.7095\n",
            "\n",
            "Epoch 67/200\n",
            "391/391 - 16s - loss: 0.5021 - accuracy: 0.8159 - val_loss: 0.9146 - val_accuracy: 0.7088\n",
            "\n",
            "Epoch 68/200\n",
            "391/391 - 16s - loss: 0.4902 - accuracy: 0.8198 - val_loss: 0.9418 - val_accuracy: 0.7018\n",
            "\n",
            "Epoch 69/200\n",
            "391/391 - 16s - loss: 0.4833 - accuracy: 0.8239 - val_loss: 0.9397 - val_accuracy: 0.7090\n",
            "\n",
            "Epoch 70/200\n",
            "391/391 - 16s - loss: 0.4726 - accuracy: 0.8276 - val_loss: 0.9162 - val_accuracy: 0.7058\n",
            "\n",
            "Epoch 71/200\n",
            "391/391 - 16s - loss: 0.4615 - accuracy: 0.8294 - val_loss: 0.9539 - val_accuracy: 0.7069\n",
            "\n",
            "Epoch 72/200\n",
            "391/391 - 16s - loss: 0.4572 - accuracy: 0.8295 - val_loss: 0.9404 - val_accuracy: 0.6998\n",
            "\n",
            "Epoch 73/200\n",
            "391/391 - 16s - loss: 0.4421 - accuracy: 0.8374 - val_loss: 0.9423 - val_accuracy: 0.7070\n",
            "\n",
            "Epoch 74/200\n",
            "391/391 - 16s - loss: 0.4390 - accuracy: 0.8376 - val_loss: 0.9430 - val_accuracy: 0.7092\n",
            "\n",
            "Epoch 75/200\n",
            "391/391 - 16s - loss: 0.4288 - accuracy: 0.8419 - val_loss: 0.9455 - val_accuracy: 0.7050\n",
            "\n",
            "Epoch 76/200\n",
            "391/391 - 16s - loss: 0.4210 - accuracy: 0.8474 - val_loss: 0.9655 - val_accuracy: 0.7077\n",
            "\n",
            "Epoch 77/200\n",
            "391/391 - 16s - loss: 0.4120 - accuracy: 0.8478 - val_loss: 0.9679 - val_accuracy: 0.7059\n",
            "\n",
            "Epoch 78/200\n",
            "391/391 - 16s - loss: 0.4051 - accuracy: 0.8495 - val_loss: 0.9436 - val_accuracy: 0.7116\n",
            "\n",
            "Epoch 79/200\n",
            "391/391 - 16s - loss: 0.3989 - accuracy: 0.8527 - val_loss: 0.9806 - val_accuracy: 0.7095\n",
            "\n",
            "Epoch 80/200\n",
            "391/391 - 16s - loss: 0.3922 - accuracy: 0.8543 - val_loss: 0.9742 - val_accuracy: 0.7089\n",
            "\n",
            "Epoch 81/200\n",
            "391/391 - 16s - loss: 0.3897 - accuracy: 0.8552 - val_loss: 0.9622 - val_accuracy: 0.7087\n",
            "\n",
            "Epoch 82/200\n",
            "391/391 - 16s - loss: 0.3778 - accuracy: 0.8590 - val_loss: 0.9997 - val_accuracy: 0.7083\n",
            "\n",
            "Epoch 83/200\n",
            "391/391 - 16s - loss: 0.3685 - accuracy: 0.8625 - val_loss: 0.9884 - val_accuracy: 0.7059\n",
            "\n",
            "Epoch 84/200\n",
            "391/391 - 16s - loss: 0.3652 - accuracy: 0.8617 - val_loss: 0.9958 - val_accuracy: 0.7082\n",
            "\n",
            "Epoch 85/200\n",
            "391/391 - 16s - loss: 0.3558 - accuracy: 0.8677 - val_loss: 1.0048 - val_accuracy: 0.7111\n",
            "\n",
            "Epoch 86/200\n",
            "391/391 - 16s - loss: 0.3515 - accuracy: 0.8692 - val_loss: 1.0016 - val_accuracy: 0.7085\n",
            "\n",
            "Epoch 87/200\n",
            "391/391 - 16s - loss: 0.3512 - accuracy: 0.8683 - val_loss: 1.0246 - val_accuracy: 0.7054\n",
            "\n",
            "Epoch 88/200\n",
            "391/391 - 16s - loss: 0.3401 - accuracy: 0.8732 - val_loss: 1.0078 - val_accuracy: 0.7051\n",
            "\n",
            "Epoch 89/200\n",
            "391/391 - 16s - loss: 0.3375 - accuracy: 0.8758 - val_loss: 1.0530 - val_accuracy: 0.7082\n",
            "\n",
            "Epoch 90/200\n",
            "391/391 - 16s - loss: 0.3331 - accuracy: 0.8762 - val_loss: 1.0320 - val_accuracy: 0.7090\n",
            "\n",
            "Epoch 91/200\n",
            "391/391 - 16s - loss: 0.3300 - accuracy: 0.8774 - val_loss: 1.0232 - val_accuracy: 0.7018\n",
            "\n",
            "Epoch 92/200\n",
            "391/391 - 16s - loss: 0.3220 - accuracy: 0.8804 - val_loss: 1.0207 - val_accuracy: 0.7063\n",
            "\n",
            "Epoch 93/200\n",
            "391/391 - 16s - loss: 0.3119 - accuracy: 0.8853 - val_loss: 1.0773 - val_accuracy: 0.7033\n",
            "\n",
            "Epoch 94/200\n",
            "391/391 - 16s - loss: 0.3063 - accuracy: 0.8848 - val_loss: 1.0787 - val_accuracy: 0.7074\n",
            "\n",
            "Epoch 95/200\n",
            "391/391 - 16s - loss: 0.3052 - accuracy: 0.8885 - val_loss: 1.0394 - val_accuracy: 0.7081\n",
            "\n",
            "Epoch 96/200\n",
            "391/391 - 16s - loss: 0.2959 - accuracy: 0.8909 - val_loss: 1.0675 - val_accuracy: 0.7092\n",
            "\n",
            "Epoch 97/200\n",
            "391/391 - 16s - loss: 0.2948 - accuracy: 0.8902 - val_loss: 1.0731 - val_accuracy: 0.7115\n",
            "\n",
            "Epoch 98/200\n",
            "391/391 - 16s - loss: 0.2862 - accuracy: 0.8945 - val_loss: 1.0903 - val_accuracy: 0.7087\n",
            "\n",
            "Epoch 99/200\n",
            "391/391 - 16s - loss: 0.2850 - accuracy: 0.8936 - val_loss: 1.0776 - val_accuracy: 0.7078\n",
            "\n",
            "Epoch 100/200\n",
            "391/391 - 16s - loss: 0.2828 - accuracy: 0.8947 - val_loss: 1.1472 - val_accuracy: 0.7062\n",
            "\n",
            "Epoch 101/200\n",
            "391/391 - 16s - loss: 0.2774 - accuracy: 0.8980 - val_loss: 1.1166 - val_accuracy: 0.7086\n",
            "\n",
            "Epoch 102/200\n",
            "391/391 - 16s - loss: 0.2748 - accuracy: 0.8972 - val_loss: 1.1094 - val_accuracy: 0.7066\n",
            "\n",
            "Epoch 103/200\n",
            "391/391 - 16s - loss: 0.2721 - accuracy: 0.8983 - val_loss: 1.1049 - val_accuracy: 0.7082\n",
            "\n",
            "Epoch 104/200\n",
            "391/391 - 16s - loss: 0.2667 - accuracy: 0.9011 - val_loss: 1.1252 - val_accuracy: 0.7114\n",
            "\n",
            "Epoch 105/200\n",
            "391/391 - 16s - loss: 0.2621 - accuracy: 0.9017 - val_loss: 1.1260 - val_accuracy: 0.7098\n",
            "\n",
            "Epoch 106/200\n",
            "391/391 - 16s - loss: 0.2586 - accuracy: 0.9037 - val_loss: 1.1144 - val_accuracy: 0.7108\n",
            "\n",
            "Epoch 107/200\n",
            "391/391 - 16s - loss: 0.2502 - accuracy: 0.9064 - val_loss: 1.1308 - val_accuracy: 0.7120\n",
            "\n",
            "Epoch 108/200\n",
            "391/391 - 16s - loss: 0.2527 - accuracy: 0.9050 - val_loss: 1.1285 - val_accuracy: 0.7090\n",
            "\n",
            "Epoch 109/200\n",
            "391/391 - 16s - loss: 0.2463 - accuracy: 0.9077 - val_loss: 1.1747 - val_accuracy: 0.7099\n",
            "\n",
            "Epoch 110/200\n",
            "391/391 - 16s - loss: 0.2460 - accuracy: 0.9073 - val_loss: 1.1270 - val_accuracy: 0.7111\n",
            "\n",
            "Epoch 111/200\n",
            "391/391 - 16s - loss: 0.2377 - accuracy: 0.9111 - val_loss: 1.1612 - val_accuracy: 0.7045\n",
            "\n",
            "Epoch 112/200\n",
            "391/391 - 16s - loss: 0.2370 - accuracy: 0.9113 - val_loss: 1.1691 - val_accuracy: 0.7090\n",
            "\n",
            "Epoch 113/200\n",
            "391/391 - 16s - loss: 0.2360 - accuracy: 0.9113 - val_loss: 1.1793 - val_accuracy: 0.7039\n",
            "\n",
            "Epoch 114/200\n",
            "391/391 - 16s - loss: 0.2295 - accuracy: 0.9132 - val_loss: 1.1633 - val_accuracy: 0.7091\n",
            "\n",
            "Epoch 115/200\n",
            "391/391 - 16s - loss: 0.2270 - accuracy: 0.9154 - val_loss: 1.1680 - val_accuracy: 0.7092\n",
            "\n",
            "Epoch 116/200\n",
            "391/391 - 16s - loss: 0.2302 - accuracy: 0.9131 - val_loss: 1.1733 - val_accuracy: 0.7112\n",
            "\n",
            "Epoch 117/200\n",
            "391/391 - 16s - loss: 0.2238 - accuracy: 0.9181 - val_loss: 1.1873 - val_accuracy: 0.7085\n",
            "\n",
            "Epoch 118/200\n",
            "391/391 - 16s - loss: 0.2191 - accuracy: 0.9188 - val_loss: 1.2210 - val_accuracy: 0.7135\n",
            "\n",
            "Epoch 119/200\n",
            "391/391 - 16s - loss: 0.2179 - accuracy: 0.9180 - val_loss: 1.2183 - val_accuracy: 0.7108\n",
            "\n",
            "Epoch 120/200\n",
            "391/391 - 16s - loss: 0.2124 - accuracy: 0.9215 - val_loss: 1.1739 - val_accuracy: 0.7100\n",
            "\n",
            "Epoch 121/200\n",
            "391/391 - 16s - loss: 0.2112 - accuracy: 0.9217 - val_loss: 1.2069 - val_accuracy: 0.7139\n",
            "\n",
            "Epoch 122/200\n",
            "391/391 - 16s - loss: 0.2102 - accuracy: 0.9227 - val_loss: 1.1932 - val_accuracy: 0.7091\n",
            "\n",
            "Epoch 123/200\n",
            "  0%|          | 0/10 [32:14<?, ?it/s, best loss: ?]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-8f25eecb535e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                                       \u001b[0mmax_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                                       \u001b[0mnotebook_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Hyperas_main-22'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                                       trials = Trials())\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space, keep_temp)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                      keep_temp=keep_temp)\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[0;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack, keep_temp)\u001b[0m\n\u001b[1;32m    137\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m              return_argmin=True),\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/temp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[0;34m(space)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7-877n7WMMY"
      },
      "source": [
        "# Printing best models\n",
        "import pandas\n",
        "df = pandas.read_csv('hyperas-fashionmnist-log.csv', delimiter=';')\n",
        "df.sort_values(by=['best_val_acc'], ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoHH4FhYXYXp"
      },
      "source": [
        "# Print the best 10 models\n",
        "hyperas_log = pandas.read_csv('hyperas-fashionmnist-log.csv', delimiter=';')\n",
        "hyperas_best10 = hyperas_log.sort_values(by=['best_val_acc'], ascending=False).head(n=10)\n",
        "hyperas_best10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ID0FX1iZ8eA"
      },
      "source": [
        "# Print the worst 10 models\n",
        "hyperas_worst10 = hyperas_log.sort_values(by=['best_val_acc'], ascending=False).tail(n=10)\n",
        "hyperas_worst10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTxrtrmmYilP"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for hyperparam in [ 'n_filters1', 'n_filters2', 'n_kernel1', 'n_kernel2', 'dropout_1', 'dropout_2', 'n_batch']:\n",
        "  ax1 = hyperas_log.plot(kind='scatter', x=hyperparam, y='best_val_acc')\n",
        "  hyperas_best10.plot(kind='scatter', x=hyperparam, y='best_val_acc', color='red', ax=ax1)\n",
        "  hyperas_worst10.plot(kind='scatter', x=hyperparam, y='best_val_acc', color='yellow', ax=ax1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8BhiojwXknO"
      },
      "source": [
        "plt.scatter(hyperas_log.optim, hyperas_log.best_val_acc)\n",
        "plt.scatter(hyperas_best10.optim, hyperas_best10.best_val_acc, color='red')\n",
        "plt.scatter(hyperas_worst10.optim, hyperas_worst10.best_val_acc, color='yellow')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3pomj1ybGXP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ho5bSptat-4"
      },
      "source": [
        "# How are the hyperparameters and the val_acc realated?\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "# Let's plot the relation of n_filters1 / n_filters2 and the best_val_acc\n",
        "fig = plt.figure(figsize=(10,7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "p = ax.scatter3D(hyperas_log.n_filters1, hyperas_log.n_filters2, hyperas_log.best_val_acc, c=hyperas_log.best_val_acc)\n",
        "ax.set_xlabel('n_filters1')\n",
        "ax.set_ylabel('n_filters2')\n",
        "ax.set_zlabel('best_val_acc')\n",
        "fig.colorbar(p)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7h3x95zbzYF"
      },
      "source": [
        "# Let's plot the relation of dropout_1 / dropout_2 and the best_val_acc\n",
        "fig = plt.figure(figsize=(10,7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "p = ax.scatter3D(hyperas_log.dropout_1, hyperas_log.dropout_2, hyperas_log.best_val_acc, c=hyperas_log.best_val_acc)\n",
        "ax.set_xlabel('dropout_1')\n",
        "ax.set_ylabel('dropout_1')\n",
        "ax.set_zlabel('best_val_acc')\n",
        "fig.colorbar(p)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltzHwtVDb2yI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}